# @package _global_

# Basic settings
defaults:
  - model
  - trainer
  - dataset: libero_object
  - simulation
  - image_encoders: resnet
  - language_encoders: clip
  - _self_

model_name: mamba
group: MambaVLA
seed: 0

# Wandb configuration
wandb:
  entity: sainavaneet
  project: MambaVLA

# Shape metadata
shape_meta:
  obs:
    agentview_image:
      shape: [3, 128, 128]
      type: rgb
    eye_in_hand_image:
      shape: [3, 128, 128]
      type: rgb

# Training parameters
train_batch_size: 32
val_batch_size: 32
num_workers: 0
device: cuda
epoch: 1
eval_every_n_epochs: 1
scale_data: true
scaling_type: minmax

# Environment parameters
obs_dim: 9
action_dim: 7
state_dim: 9
max_len_data: 260

# Observations
consider_robot_states: false
camera_names:
  - agentview
  - eye_in_hand

# Model parameters
chunck_size: 10
perception_seq_len: 1
action_seq_len: 10
multistep: 10

goal_conditioned: true
use_pos_emb: true
num_sampling_steps: 4
if_use_ema: true
obs_tokens: 2

# Architecture parameters
len_embd: 256
lang_emb_dim: 512
latent_dim: 256

n_heads: 4
mamba_encoder_cfg:
  layer: Mamba1
  d_state: 64
  d_conv: 5
  expand: 2
mamba_n_layer_encoder: 5
